
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>HDFS文件操作 | 刘兴的博客</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="刘兴">
    

    
    <meta name="description" content="HDFS是一种文件系统，专为MapReduce这类框架下的大规模分布式数据处理而设计。你可以把一个大数据集（比如说100TB）在HDFS中存储为单个文件，而大多数其他的文件系统无力实现这一点。HDFS使你不必考虑这些细节，让你感觉就像在处理单个文件一样。">
<meta property="og:type" content="article">
<meta property="og:title" content="HDFS文件操作">
<meta property="og:url" content="http://pangjiuzala.github.io/2015/08/03/HDFS文件操作/index.html">
<meta property="og:site_name" content="刘兴的博客">
<meta property="og:description" content="HDFS是一种文件系统，专为MapReduce这类框架下的大规模分布式数据处理而设计。你可以把一个大数据集（比如说100TB）在HDFS中存储为单个文件，而大多数其他的文件系统无力实现这一点。HDFS使你不必考虑这些细节，让你感觉就像在处理单个文件一样。">
<meta property="og:updated_time" content="2015-08-02T13:01:45.814Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HDFS文件操作">
<meta name="twitter:description" content="HDFS是一种文件系统，专为MapReduce这类框架下的大规模分布式数据处理而设计。你可以把一个大数据集（比如说100TB）在HDFS中存储为单个文件，而大多数其他的文件系统无力实现这一点。HDFS使你不必考虑这些细节，让你感觉就像在处理单个文件一样。">

    
    <link rel="alternative" href="https://github.com/search?q=pangjiuzala&type=Users" title="刘兴的博客" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="刘兴的博客" title="刘兴的博客"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="刘兴的博客">刘兴的博客</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">主页</a></li>
					
						<li><a href="/archives">文章列表</a></li>
					
					<li>
 					
					<form class="search" action="/search/index.html" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" autocomplete="off" name="q" maxlength="20" placeholder="搜索" />
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2015/08/03/HDFS文件操作/" title="HDFS文件操作" itemprop="url">HDFS文件操作</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="刘兴" target="_blank" itemprop="author">刘兴</a>
		
  <p class="article-time">
    <time datetime="2015-08-03T00:00:23.000Z" itemprop="datePublished"> 发表于 2015-08-03</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#基本文件命令"><span class="toc-number">1.</span> <span class="toc-text">基本文件命令</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#指定文件盒目录确切位置的URI"><span class="toc-number">2.</span> <span class="toc-text">指定文件盒目录确切位置的URI</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#添加文件和目录"><span class="toc-number">3.</span> <span class="toc-text">添加文件和目录</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#检索文件"><span class="toc-number">4.</span> <span class="toc-text">检索文件</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#删除文件"><span class="toc-number">5.</span> <span class="toc-text">删除文件</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#编程读写HDFS"><span class="toc-number">6.</span> <span class="toc-text">编程读写HDFS</span></a></li></ol>
		
		</div>
		
		<p>HDFS是一种文件系统，专为MapReduce这类框架下的大规模分布式数据处理而设计。你可以把一个大数据集（比如说100TB）在HDFS中存储为单个文件，而大多数其他的文件系统无力实现这一点。HDFS使你不必考虑这些细节，让你感觉就像在处理单个文件一样。<br><a id="more"></a><br>因为HDFS并不是一个天生的Unix文件系统，不支持像ls和cp这种标准的Unix文件命令，也不支持如fopen（）和fread（）这样的标准文件读写操作。另一方面，Hadoop确也提供了一套与linux文件命令类似的命令行工具。<br>注意 一个典型的Hadoop工作流会在别的地方生成数据库文件(如日志文件)再将其复制到HDFS中。接着由MapReduce程序处理这个数据，但它们通常不会直接读任何一个HDFS文件。相反，它们依靠MapReduce框架来读取HDFS文件，并将之解析为独立的记录（键值对），这些记录才是MapReduce程序所处理的数据单元。除非需要定制数据的导入与导出，否则你几乎不必编程来读写HDFS文件。</p>
<h1 id="基本文件命令">基本文件命令</h1><p>Hadoop的文件命令采取的形式为</p>
<figure class="highlight dos"><table><tr><td class="code"><pre><span class="line">hadoop <span class="built_in">fs</span> -<span class="built_in">cmd</span> &lt;args&gt;</span><br></pre></td></tr></table></figure>
<p>其中cmd是具体的文件命令，而<args>是一组数目可变的命令参数cmd命令通常与UNIX对应的命令名相同。例如，文件列表命令为</args></p>
<figure class="highlight dos"><table><tr><td class="code"><pre><span class="line">hadoop <span class="built_in">fs</span> -ls</span><br></pre></td></tr></table></figure>
<p>让我们来看看在Hadoop中最常用文件管理任务，其中包括<br><strong>添加文件和目录</strong><br><strong>获取文件</strong><br><strong>删除文件</strong></p>
<h1 id="指定文件盒目录确切位置的URI">指定文件盒目录确切位置的URI</h1><p>Hadoop的文件命令既可以与HDFS文件系统交互，也可以和本地文件系统交互。URI精确地定位到一个特定文件或目录的位置。完整的URI格式为<code>scheme://authority/path</code>。Scheme类似于一个协议。它可以是hdfs或file，来分别指定HDFS文件系统或本地文件系统。对于HDFS，authority是NameNode的主机名，而path是文件或目录的路径。例如，对于在本地机器的9000端口号上，以标准伪分布式模型运行的HDFS，访问用户目录user/chuck中文件example.txt的URI大致为<code>hdfs://localhost:9000/user/chuck/example.txt</code>.<br>你可以使用hadoop的cat命令来显示该文件的内容:<br><code>hadoop fs -cat hdfs://localhost:9000/usr/chuck/example.txt</code><br>正如我们马上就会看到的，大多数设置不需要指定URI中的scheme://authority部分。对于本地文件系统，你可能会更喜欢用标准的Unix命令，而不是hadoop文件命令。当在本地文件系统和HDFS之间复制文件时，hadoop中的命令（如put和get）会分别把本地文件系统作为源和目的，而不需要制定scheme为file。对于其他命令，如果未设置URI中scheme://authority,就会采用Hadoop的默认配置。例如，假如conf/core-site.xml文件已经更改为伪分布式配置，则文件中fs.default.name属性应为</p>
<figure class="highlight pf"><table><tr><td class="code"><pre><span class="line"><span class="variable">&lt;property&gt;</span></span><br><span class="line">	<span class="variable">&lt;name&gt;</span>fs.<span class="keyword">default</span>.name<span class="variable">&lt;/name&gt;</span></span><br><span class="line">	<span class="variable">&lt;value&gt;</span>hdfs://localhost:<span class="number">9000</span><span class="variable">&lt;/value&gt;</span></span><br><span class="line"><span class="variable">&lt;/property&gt;</span></span><br></pre></td></tr></table></figure>
<p>在此配置下,URI<code>hdfs://localhost:9000/user/chuck/example.txt</code>缩短为<code>/user/chuck/example.txt</code>。显示文件内容的hadoo cat命令可以写为<code>hadoop fs -cat example.txt</code></p>
<h1 id="添加文件和目录">添加文件和目录</h1><p>在运行hadoop程序处理存储在HDFS上的数据之前，你需要首先把数据放在HDFS上。让我们假设你已经完成了格式化，并启动了一个HDFS文件系统（出于学习目的，我们建议使用伪分布式模式的配置。）让我们创建一个目录并放入一个文件。<br>HDFS有一个默认的工作目录/usr/$USER,其中$USER是你的登录用户名。你需要用你的用户名来替换。</p>
<figure class="highlight dos"><table><tr><td class="code"><pre><span class="line">hadoop <span class="built_in">fs</span> -<span class="built_in">mkdir</span> /user/chuck</span><br></pre></td></tr></table></figure>
<p>hadoop的mkdir命令会自动创建父目录（如果此前不存在的话），类似于UNIX中使用-p选项的mkdir命令，因此上述命令还会创建/user目录，让我们用ls命令对目录进行检查:</p>
<figure class="highlight dos"><table><tr><td class="code"><pre><span class="line">hadoop <span class="built_in">fs</span> -ls/</span><br></pre></td></tr></table></figure>
<p>该命令返回根目录下目录/user的信息</p>
<figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line"><span class="label">Found</span> <span class="number">1</span> <span class="keyword">items</span><br><span class="line"></span><span class="label">drwxr</span>-xr-x -chuck supergroup <span class="number">0</span> <span class="number">2009</span>-<span class="number">01</span>-<span class="number">14</span> <span class="number">10</span>:<span class="number">23</span> /user</span><br></pre></td></tr></table></figure>
<p>如果你想看到所有的子目录，则可以使用Hadoop的lsr命令，它类似于Unix中打开-r选项的<code>ls:hadoop fs -lsr /</code><br>你会看到所有文件和子目录：</p>
<figure class="highlight dns"><table><tr><td class="code"><pre><span class="line">drwxr-xr-x -chuck supergroup <span class="number">0 2009-01</span>-14 10:23 /user</span><br><span class="line">drwxr-xr-x -chuck supergroup <span class="number">0 2009-01</span>-14 10:23 /user/chuck</span><br></pre></td></tr></table></figure>
<p>既然有了一个工作目录，我们可以放一个文件进去。在本地文件系统中创建一个名为example.txt的文本文件，用hadoop的命令put将它从本地文件系统复制到HDFS中去。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">hadoop fs -put example<span class="class">.txt</span>.</span><br></pre></td></tr></table></figure>
<p>注意上面的这个命令最后一个参数是句点(.)。这意味着我们把文件放入了默认的工作目录。该命令等价于:</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">hadoop fs -put example<span class="class">.txt</span> /user/chuck</span><br></pre></td></tr></table></figure>
<p>我们重新执行递归列出文件的命令，可以看到新的文件被添加到HDFS中。</p>
<figure class="highlight dns"><table><tr><td class="code"><pre><span class="line">$hadoop fs -lsr /</span><br><span class="line">drwxr-xr-x -chuck supergroup <span class="number">0 2009-01</span>-14 10:23 /user</span><br><span class="line">drwxr-xr-x -chuck supergroup <span class="number">0 2009-01</span>-14 11:02 /user/chuck</span><br><span class="line">-rw-r--r-- 1 chuck supergroup <span class="number">264 2009</span>-<span class="number">01-14 11:02</span> /user/chuck/example.txt</span><br></pre></td></tr></table></figure>
<p>实际上，我们并不需要递归地检查所有文件，而仅限于在我们自己工作目录中的文件。我们可以通过最简单的形式来使用Hadoop的ls命令：</p>
<figure class="highlight brainfuck"><table><tr><td class="code"><pre><span class="line"><span class="comment">$</span> <span class="comment">hadoop</span> <span class="comment">fs</span> <span class="literal">-</span><span class="comment">ls</span></span><br><span class="line"><span class="comment">Found</span> <span class="comment">1</span> <span class="comment">items</span></span><br><span class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">1</span> <span class="comment">chuck</span> <span class="comment">supergroup</span> <span class="comment">264</span> <span class="comment">2009</span><span class="literal">-</span><span class="comment">01</span><span class="literal">-</span><span class="comment">14</span> <span class="comment">11:02</span> <span class="comment">/user/chuck/exampe</span><span class="string">.</span><span class="comment">txt</span></span><br></pre></td></tr></table></figure>
<p>输出结果显示出属性信息，比如权限、所有者、组、文件大小，以及最后修改日期，所有这些都类似于Unix的概念。显示“1”的列给出文件的复制因子。伪分布式配置下它永远为1。对于生产环境中的集群，复制因子通常为3，但也可以是任何正整数。因为复制因子不适用于目录，故届时该列会显示一个破折号（-）。<br>当你把数据放到HDFS上之后，你可以运行Hadoop程序来处理它。处理过程将输出一组新的HDFS文件，然后你可以读取或检索这些结果。</p>
<h1 id="检索文件">检索文件</h1><p>hadoop的get命令与put截然相反。它从HDFS中复制文件到本地文件系统。比方说，我们在本地不再拥有文件example.txt，而想从HDFS中将它取回，我们就可以运行命令:</p>
<p><code>hadoop fs -get example.txt.</code><br>将它复制到我们在本地的当前工作目录中。<br>另一种访问数据的方式是显示。由hadoop的cat命令来支持：<br><code>hadoop fs -cat example.txt</code><br>我们可以在Hadoop的文件命令中使用Unix的管道，将其结果发送给其他的Unix命令做进一步处理。例如，如果该文件非常大（正如典型的Hadoop文件那样），并且你希望快速地检查其内容，就可以把hadoop中cat命令的输出用管道传递给Unix命令head：<br><code>hadoop fs -cat example.txt|head</code><br>hadoop内在支持tail命令来查看最后一千字节：<br><code>hadoop fs -tail example.txt</code><br>文件在HDFS上的任务完成之后，可以考虑删除它们以释放空间。</p>
<h1 id="删除文件">删除文件</h1><p>hadoop删除文件的命令为rm，现在你也许不会感到太惊讶了：<br><code>hadoop fs -rm example.txt</code><br>rm命令还可以用于删除空目录。</p>
<h1 id="编程读写HDFS">编程读写HDFS</h1><p>为了体验HDFS的Java API，我们将开发一个PutMerge程序，用于合并文件后放入HDFS。命令行工具并不支持这个操作，我们将使用API来实现。<br>需要分析来自许多Web服务器的Apache日志文件时，就有了建立这个历程的动机。虽然我们可以把每个日志文件都复制到HDFS中，但通常而言，Hadoop处理单个大文件会比处理许多个小文件更有效率（这里“小”是相对的，因为它仍会达到几十或几百GB)。此外，从分析目的来看，我们把日志数据视为一个大文件。日志数据被分散在多个文件是由于Web服务器采用分布式架构所带来的副作用。一种解决办法是先将所有的文件合并，然后再复制到HDFS。可是，文件合并需要占用本地计算机的大量磁盘空间。如果我们能够在向HDFS复制的过程中合并它们，事情就会简单很多。<br>因此，我们需要一个PutMerge类型的操作。hadoo命令行中有一个getmerge命令，用于一组HDFS文件在复制到本地以前进行合并。但我们想要的截然相反，故无法再Hadoop的文件管理工具中获得。我们用HDFS API来自己编程实现它。<br>在Hadoop中用作文件操作的主流位于org.apache.hadoop.fs软件包中。Hadoop的基本文件操作包括常见的open、read、write和close。实际上，hadoop的文件API是通用的，可以用于HDFS以外的其他文件系统。对于我们的PutMerge程序，它读取本地文件系统和写入HDFS都会使用Hadoop的文件api。</p>
<p>不多说，直接上代码:</p>
<figure class="highlight actionscript"><table><tr><td class="code"><pre><span class="line"><span class="preprocessor"><span class="keyword">import</span> java.io.IOException;</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span></span><br><span class="line"><span class="preprocessor"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span></span><br><span class="line"><span class="preprocessor"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span></span><br><span class="line"><span class="preprocessor"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</span></span><br><span class="line"><span class="preprocessor"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span></span><br><span class="line"><span class="preprocessor"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span></span><br><span class="line"><span class="preprocessor"><span class="keyword">import</span> org.eclipse.jdt.internal.compiler.ConfigurableOption;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PutMerge</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(String[] args) throws IOException</span><br><span class="line">    &#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem hdfs = FileSystem.<span class="keyword">get</span>(conf);</span><br><span class="line">        FileSystem local = FileSystem.getLocal(conf);</span><br><span class="line">        Path inputDir = <span class="keyword">new</span> Path(args[<span class="number">0</span>]);<span class="comment">//1.设定输入目录与输出文件</span></span><br><span class="line">        Path hdfsFile = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">try</span></span><br><span class="line">        &#123;</span><br><span class="line">            FileStatus[] inputFiles = local.listStatus(inputDir);<span class="comment">//2.得到本地文件列表</span></span><br><span class="line">            FSDataOutputStream out = hdfs.create(hdfsFile);<span class="comment">//3.生成HDFS文件流</span></span><br><span class="line">            <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; inputFiles.length; i++)</span><br><span class="line">            &#123;</span><br><span class="line">                System.out.println(inputFiles[i].getPath().getName());</span><br><span class="line">                FSDataInputStream <span class="keyword">in</span> = local.open(inputFiles[i].getPath());<span class="comment">//4.打开本地输入流</span></span><br><span class="line">                byte buffer[] = <span class="keyword">new</span> byte[<span class="number">256</span>];</span><br><span class="line">                int bytesRead = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">while</span> ((bytesRead = <span class="keyword">in</span>.read(buffer)) &gt; <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    out.write(buffer, <span class="number">0</span>, bytesRead);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">in</span>.close();</span><br><span class="line">            &#125;</span><br><span class="line">            out.close();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (Exception e)</span><br><span class="line">        &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们已经讨论了如何处理HDFS中的文件，你现在知道一些方法来读写HDFS中的数据，但是仅仅有数据还不够，还要对它进行处理分析以及其他的操作。在后续学习过程中，我们将介绍Hadoop的另一个主要组件——MapReduce框架，看看如何基于它来编程。</p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/Hadoop/">Hadoop</a><a href="/tags/大数据/">大数据</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	<div class="share-jiathis">
	  
<div class="jiathis_style_24x24">
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_renren"></a>
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_googleplus"></a>
	<a class="jiathis_button_douban"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
    var jiathis_config={
    data_track_clickback:true,
    sm:"copy,renren,cqq",
    pic:"",
    summary:"",
    
  </script> 
<script type="text/javascript" src="//v3.jiathis.com/code/jia.js?uid=
" charset="utf-8"></script>      

	 </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2015/08/04/剖析Mapeduce程序/" title="剖析Mapeduce程序">
  <strong>上一篇：</strong><br/>
  <span>
  剖析Mapeduce程序</span>
</a>
</div>


<div class="next">
<a href="/2015/08/02/深入理解Hadoop集群和网络/"  title="深入理解Hadoop集群和网络">
 <strong>下一篇：</strong><br/> 
 <span>深入理解Hadoop集群和网络
</span>
</a>
</div>

</nav>

	
<section id="comments" class="comment">
	<div class="ds-thread" data-thread-key="2015/08/03/HDFS文件操作/" data-title="HDFS文件操作" data-url="http://pangjiuzala.github.io/2015/08/03/HDFS文件操作/"></div>
</section>


</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#基本文件命令"><span class="toc-number">1.</span> <span class="toc-text">基本文件命令</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#指定文件盒目录确切位置的URI"><span class="toc-number">2.</span> <span class="toc-text">指定文件盒目录确切位置的URI</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#添加文件和目录"><span class="toc-number">3.</span> <span class="toc-text">添加文件和目录</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#检索文件"><span class="toc-number">4.</span> <span class="toc-text">检索文件</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#删除文件"><span class="toc-number">5.</span> <span class="toc-text">删除文件</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#编程读写HDFS"><span class="toc-number">6.</span> <span class="toc-text">编程读写HDFS</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  <div class="weiboshow">
  <p class="asidetitle">新浪微博</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=2129798793&verifier=c0951e84&dpc=1"></iframe>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Java/" title="Java">Java<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/算法/" title="算法">算法<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/数据挖掘/" title="数据挖掘">数据挖掘<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/大数据/" title="大数据">大数据<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/Hadoop/" title="Hadoop">Hadoop<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/移动互联网/" title="移动互联网">移动互联网<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/物联网/" title="物联网">物联网<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Android/" title="Android">Android<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/云平台/" title="云平台">云平台<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/云计算/" title="云计算">云计算<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/设计模式/" title="设计模式">设计模式<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Python/" title="Python">Python<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/网络爬虫/" title="网络爬虫">网络爬虫<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/操作系统/" title="操作系统">操作系统<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/OpenStack/" title="OpenStack">OpenStack<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/智慧医疗/" title="智慧医疗">智慧医疗<sup>1</sup></a></li>
			
		
		</ul>
</div>


  
  <div class="archiveslist">
    <p class="asidetitle"><a href="/archives">归档</a></p>
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">25</span></li></ul>
  </div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="http://blog.csdn.net/pangjiuzala" target="_blank" title="刘兴的CSDN博客">CSDN</a>
            
          </li>
        
    </ul>
</div>

  <div class="rsspart">
	<a href="https://github.com/search?q=pangjiuzala&amp;type=Users" target="_blank" title="关注刘兴的github">关注</a>
</div>

  

  


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello,I&#39;m from ZjuCs! <br/>
			The more you diligent, the more you lucky!</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Copyright@ 2015 Liuxing All rights reserved.
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>




<script type="text/javascript">
  var duoshuoQuery = {short_name:"pangjiuzala"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 







<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->

<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'null', 'null');  
ga('send', 'pageview');
</script>



<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Ffeafc504b70a541dd3845d467335f367' type='text/javascript'%3E%3C/script%3E"));
</script>



<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_null'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s23.cnzz.com/z_stat.php%3Fid%3Dnull' type='text/javascript'%3E%3C/script%3E"));</script>

<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<script>
var option = {
  engineKey: '4ac092ad8d749fdc6293'
};
(function(w,d,t,u,n,s,e){
  s = d.createElement(t);
  s.src = u;
  s.async = 1;
  w[n] = function(r){
    w[n].opts = r;
  };
  e = d.getElementsByTagName(t)[0];
  e.parentNode.insertBefore(s, e);
})(window,document,'script','//tinysou-cdn.b0.upaiyun.com/ts.js','_ts');
_ts(option);
</script>

<!-- Tiny_search End -->

  </body>
</html>
